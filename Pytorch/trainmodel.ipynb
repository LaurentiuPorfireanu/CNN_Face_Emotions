{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19ce7749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T09:52:11.045886Z",
     "start_time": "2025-04-21T09:52:11.041412Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from typing import List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e9c221",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T09:52:11.057822Z",
     "start_time": "2025-04-21T09:52:11.051885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = 'images/train'\n",
    "TEST_DIR = 'images/test'\n",
    "IMAGE_SIZE = 96\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 0.0016\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)\n",
    "\n",
    "EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3ba4203d84b610",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T09:52:11.067536Z",
     "start_time": "2025-04-21T09:52:11.063885Z"
    }
   },
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "\t\"\"\"\n",
    "\tCustom PyTorch Dataset for loading facial emotion images.\n",
    "\n",
    "\tEach sample consists of:\n",
    "\t- A grayscale image loaded from a file path.\n",
    "\t- An associated emotion label.\n",
    "\n",
    "\tArgs:\n",
    "\t\timage_paths (List[str]): List of file paths to the images.\n",
    "\t\tlabels (List[int]): List of corresponding emotion labels.\n",
    "\t\ttransform (Optional[callable], optional): Optional transformation to be applied on an image.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, image_paths: List[str], labels: List[int], transform: Optional[callable] = None) -> None:\n",
    "\t\tself.image_paths = image_paths\n",
    "\t\tself.labels = labels\n",
    "\t\tself.transform = transform\n",
    "\n",
    "\tdef __len__(self) -> int:\n",
    "\t\t\"\"\"\n",
    "\t\tReturns the total number of samples in the dataset.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tint: Number of samples.\n",
    "\t\t\"\"\"\n",
    "\t\treturn len(self.image_paths)\n",
    "\n",
    "\tdef __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "\t\t\"\"\"\n",
    "\t\tRetrieves the image and label at the specified index.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tidx (int): Index of the sample to retrieve.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tTuple[torch.Tensor, int]: A tuple containing the image tensor and its label.\n",
    "\t\t\"\"\"\n",
    "\t\t# Load image in grayscale mode\n",
    "\t\timg_path: str = self.image_paths[idx]\n",
    "\t\timage: Image.Image = Image.open(img_path).convert('L')\n",
    "\n",
    "\t\t# Apply transformations if provided\n",
    "\t\tif self.transform:\n",
    "\t\t\timage = self.transform(image)\n",
    "\n",
    "\t\tlabel: int = self.labels[idx]\n",
    "\n",
    "\t\treturn image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ede9c1f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T09:52:11.083867Z",
     "start_time": "2025-04-21T09:52:11.080864Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_dataframe(dir: str) -> Tuple[List[str], List[str]]:\n",
    "\t\"\"\"\n",
    "\tScans a directory structure and creates lists of image file paths and their corresponding labels.\n",
    "\n",
    "\tExpected directory structure:\n",
    "\t\tdir/\n",
    "\t\t\t|-- label_1/\n",
    "\t\t\t|     |-- img1.jpg\n",
    "\t\t\t|     |-- img2.jpg\n",
    "\t\t\t|-- label_2/\n",
    "\t\t\t|     |-- img3.jpg\n",
    "\t\t\t|     |-- img4.jpg\n",
    "\n",
    "\tArgs:\n",
    "\t\tdir (str): Path to the root directory containing subdirectories for each label.\n",
    "\n",
    "\tReturns:\n",
    "\t\tTuple[List[str], List[str]]: \n",
    "\t\t\t- List of full image paths.\n",
    "\t\t\t- List of corresponding labels (as strings).\n",
    "\t\"\"\"\n",
    "\timage_paths: List[str] = []\n",
    "\tlabels: List[str] = []\n",
    "\n",
    "\t\n",
    "\tfor label in os.listdir(dir):\n",
    "\t\tlabel_dir: str = os.path.join(dir, label)\n",
    "\t\tif os.path.isdir(label_dir):  \n",
    "\t\t\tfor image_name in os.listdir(label_dir):\n",
    "\t\t\t\t# Add full image path and corresponding label\n",
    "\t\t\t\timage_paths.append(os.path.join(label_dir, image_name))\n",
    "\t\t\t\tlabels.append(label)\n",
    "\t\t\tprint(f\"{label} completed\")\n",
    "\n",
    "\treturn image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8909e632f83686e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T09:52:11.090224Z",
     "start_time": "2025-04-21T09:52:11.085869Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_label_idx(label_name: str) -> int:\n",
    "\t\"\"\"\n",
    "\tReturns the index of a given emotion label based on the EMOTIONS list.\n",
    "\n",
    "\tArgs:\n",
    "\t\tlabel_name (str): Name of the emotion (e.g., 'happy', 'sad').\n",
    "\n",
    "\tReturns:\n",
    "\t\tint: Corresponding index of the emotion in the EMOTIONS list.\n",
    "\t\"\"\"\n",
    "\treturn EMOTIONS.index(label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed5efa61d240cbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T09:52:11.098594Z",
     "start_time": "2025-04-21T09:52:11.096186Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23117ef3633c802d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T09:52:11.107320Z",
     "start_time": "2025-04-21T09:52:11.103696Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_datasets() -> Tuple[DataLoader, DataLoader]:\n",
    "\t\"\"\"\n",
    "\tCreates training and testing datasets and their corresponding DataLoaders.\n",
    "\n",
    "\tProcess:\n",
    "\t- Scans training and testing directories to retrieve image paths and labels.\n",
    "\t- Converts label names to numeric indices.\n",
    "\t- Applies transformations to images.\n",
    "\t- Prepares DataLoaders for training and testing.\n",
    "\n",
    "\tReturns:\n",
    "\t\tTuple[DataLoader, DataLoader]: \n",
    "\t\t\t- DataLoader for the training dataset.\n",
    "\t\t\t- DataLoader for the testing dataset.\n",
    "\t\"\"\"\n",
    "\t# Get training data\n",
    "\ttrain_image_paths, train_label_names = create_dataframe(TRAIN_DIR)\n",
    "\ttrain_labels = [get_label_idx(label) for label in train_label_names]\n",
    "\n",
    "\t# Get testing data\n",
    "\ttest_image_paths, test_label_names = create_dataframe(TEST_DIR)\n",
    "\ttest_labels = [get_label_idx(label) for label in test_label_names]\n",
    "\n",
    "\t# Create Dataset instances\n",
    "\ttrain_dataset = EmotionDataset(train_image_paths, train_labels, transform)\n",
    "\ttest_dataset = EmotionDataset(test_image_paths, test_labels, transform)\n",
    "\n",
    "\t# Create DataLoaders\n",
    "\ttrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\ttest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\treturn train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68410c0d22248619",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T09:52:11.117881Z",
     "start_time": "2025-04-21T09:52:11.112757Z"
    }
   },
   "outputs": [],
   "source": [
    "class EmotionCNN(nn.Module):\n",
    "\t\"\"\"\n",
    "\tA Convolutional Neural Network (CNN) for emotion classification from grayscale facial images.\n",
    "\n",
    "\tArchitecture Overview:\n",
    "\t- 3 convolutional blocks (Conv2D + ReLU + MaxPooling + Dropout)\n",
    "\t- Fully connected layers for feature integration and final classification.\n",
    "\n",
    "\tArgs:\n",
    "\t\tnum_classes (int): Number of output classes (default: 7).\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, num_classes: int = 7) -> None:\n",
    "\t\tsuper(EmotionCNN, self).__init__()\n",
    "\n",
    "\t\t# First convolutional block\n",
    "\t\tself.conv1: nn.Conv2d = nn.Conv2d(in_channels=1, out_channels=128, kernel_size=3, padding=1)\n",
    "\t\tself.pool1: nn.MaxPool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\t\tself.dropout1: nn.Dropout = nn.Dropout(0.2)\n",
    "\n",
    "\t\t# Second convolutional block\n",
    "\t\tself.conv2: nn.Conv2d = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "\t\tself.pool2: nn.MaxPool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\t\tself.dropout2: nn.Dropout = nn.Dropout(0.2)\n",
    "\n",
    "\t\t# Third convolutional block\n",
    "\t\tself.conv3: nn.Conv2d = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "\t\tself.pool3: nn.MaxPool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\t\tself.dropout3: nn.Dropout = nn.Dropout(0.2)\n",
    "\n",
    "\t\t# Calculate flattened feature size\n",
    "\t\t# After 3 pooling layers (stride=2), input size reduces by a factor of 8\n",
    "\t\tself.flat_features: int = 512 * (IMAGE_SIZE // 8) * (IMAGE_SIZE // 8)\n",
    "\n",
    "\t\t# Fully connected layers\n",
    "\t\tself.fc1: nn.Linear = nn.Linear(self.flat_features, 512)\n",
    "\t\tself.dropout4: nn.Dropout = nn.Dropout(0.2)\n",
    "\t\tself.fc2: nn.Linear = nn.Linear(512, 256)\n",
    "\t\tself.dropout5: nn.Dropout = nn.Dropout(0.2)\n",
    "\t\tself.fc3: nn.Linear = nn.Linear(256, num_classes)\n",
    "\n",
    "\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\t\t\"\"\"\n",
    "\t\tDefines the forward pass of the network.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tx (torch.Tensor): Input tensor of shape (batch_size, 1, 96, 96).\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\ttorch.Tensor: Output tensor of shape (batch_size, num_classes).\n",
    "\t\t\"\"\"\n",
    "\t\t# First convolutional block\n",
    "\t\tx = F.relu(self.conv1(x))\n",
    "\t\tx = self.pool1(x)\n",
    "\t\tx = self.dropout1(x)\n",
    "\n",
    "\t\t# Second convolutional block\n",
    "\t\tx = F.relu(self.conv2(x))\n",
    "\t\tx = self.pool2(x)\n",
    "\t\tx = self.dropout2(x)\n",
    "\n",
    "\t\t# Third convolutional block\n",
    "\t\tx = F.relu(self.conv3(x))\n",
    "\t\tx = self.pool3(x)\n",
    "\t\tx = self.dropout3(x)\n",
    "\n",
    "\t\t# Flatten the tensor\n",
    "\t\tx = x.view(-1, self.flat_features)\n",
    "\n",
    "\t\t# Fully connected layers\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = self.dropout4(x)\n",
    "\t\tx = F.relu(self.fc2(x))\n",
    "\t\tx = self.dropout5(x)\n",
    "\t\tx = self.fc3(x)  \n",
    "\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b005764f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T09:52:11.128945Z",
     "start_time": "2025-04-21T09:52:11.123350Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "\tmodel: nn.Module,\n",
    "\ttrain_loader: DataLoader,\n",
    "\ttest_loader: DataLoader,\n",
    "\tcriterion: nn.Module,\n",
    "\toptimizer: torch.optim.Optimizer,\n",
    "\tnum_epochs: int\n",
    ") -> None:\n",
    "\t\"\"\"\n",
    "\tTrains and validates the model over a specified number of epochs.\n",
    "\n",
    "\tArgs:\n",
    "\t\tmodel (nn.Module): The neural network model to be trained.\n",
    "\t\ttrain_loader (DataLoader): DataLoader for the training dataset.\n",
    "\t\ttest_loader (DataLoader): DataLoader for the validation/testing dataset.\n",
    "\t\tcriterion (nn.Module): Loss function to optimize.\n",
    "\t\toptimizer (torch.optim.Optimizer): Optimizer for updating model weights.\n",
    "\t\tnum_epochs (int): Number of epochs to train the model.\n",
    "\n",
    "\tReturns:\n",
    "\t\tNone\n",
    "\t\"\"\"\n",
    "\tbest_accuracy: float = 0.0\n",
    "\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\t# Training phase\n",
    "\t\tmodel.train()\n",
    "\t\trunning_loss: float = 0.0\n",
    "\t\tcorrect: int = 0\n",
    "\t\ttotal: int = 0\n",
    "\n",
    "\t\tfor inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "\t\t\tinputs = inputs.to(DEVICE)\n",
    "\t\t\tlabels = labels.to(DEVICE)\n",
    "\n",
    "\t\t\t\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t\n",
    "\t\t\toutputs = model(inputs)\n",
    "\t\t\tloss = criterion(outputs, labels)\n",
    "\n",
    "\t\t\t\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# Update training statistics\n",
    "\t\t\trunning_loss += loss.item() * inputs.size(0)\n",
    "\t\t\t_, predicted = torch.max(outputs, 1)\n",
    "\t\t\ttotal += labels.size(0)\n",
    "\t\t\tcorrect += (predicted == labels).sum().item()\n",
    "\n",
    "\t\tepoch_loss: float = running_loss / len(train_loader.dataset)\n",
    "\t\tepoch_acc: float = correct / total\n",
    "\n",
    "\t\t# Validation phase\n",
    "\t\tmodel.eval()\n",
    "\t\tval_loss: float = 0.0\n",
    "\t\tval_correct: int = 0\n",
    "\t\tval_total: int = 0\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor inputs, labels in tqdm(test_loader, desc='Validation'):\n",
    "\t\t\t\tinputs = inputs.to(DEVICE)\n",
    "\t\t\t\tlabels = labels.to(DEVICE)\n",
    "\n",
    "\t\t\t\toutputs = model(inputs)\n",
    "\t\t\t\tloss = criterion(outputs, labels)\n",
    "\n",
    "\t\t\t\tval_loss += loss.item() * inputs.size(0)\n",
    "\t\t\t\t_, predicted = torch.max(outputs, 1)\n",
    "\t\t\t\tval_total += labels.size(0)\n",
    "\t\t\t\tval_correct += (predicted == labels).sum().item()\n",
    "\n",
    "\t\tval_loss = val_loss / len(test_loader.dataset)\n",
    "\t\tval_acc = val_correct / val_total\n",
    "\n",
    "\t\t\n",
    "\t\tprint(f'Epoch {epoch + 1}/{num_epochs}:')\n",
    "\t\tprint(f'Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f}')\n",
    "\t\tprint(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
    "\n",
    "\t\t\n",
    "\t\tif val_acc > best_accuracy:\n",
    "\t\t\tbest_accuracy = val_acc\n",
    "\t\t\ttorch.save(model.state_dict(), 'emotion_model.pth')\n",
    "\t\t\tprint(f'Model saved with accuracy: {best_accuracy:.4f}')\n",
    "\n",
    "\tprint(f'Best validation accuracy: {best_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34db366fbbfde0c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T09:52:11.137153Z",
     "start_time": "2025-04-21T09:52:11.133958Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_emotion(model: nn.Module, image_path: str) -> str:\n",
    "\t\"\"\"\n",
    "\tPredicts the emotion label for a given input image.\n",
    "\n",
    "\tArgs:\n",
    "\t\tmodel (nn.Module): Trained emotion classification model.\n",
    "\t\timage_path (str): Path to the input image file.\n",
    "\n",
    "\tReturns:\n",
    "\t\tstr: Predicted emotion label.\n",
    "\t\"\"\"\n",
    "\tmodel.eval()\n",
    "\n",
    "\t\n",
    "\timage: Image.Image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "\timage = transform(image).unsqueeze(0).to(DEVICE)  # Apply transformations and add batch dimension\n",
    "\n",
    "\t# Perform prediction\n",
    "\twith torch.no_grad():\n",
    "\t\toutputs: torch.Tensor = model(image)\n",
    "\t\t_, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "\treturn EMOTIONS[predicted.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1304bb7fb9eeb50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T10:03:26.304477Z",
     "start_time": "2025-04-21T09:52:11.142674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n",
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   3%|â–Ž         | 15/451 [00:02<01:13,  5.94it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m \t\t\u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \t\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     24\u001b[39m optimizer: optim.Optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Load best model weights\u001b[39;00m\n\u001b[32m     30\u001b[39m model.load_state_dict(torch.load(\u001b[33m'\u001b[39m\u001b[33memotion_model.pth\u001b[39m\u001b[33m'\u001b[39m, map_location=DEVICE))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, test_loader, criterion, optimizer, num_epochs)\u001b[39m\n\u001b[32m     45\u001b[39m optimizer.step()\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Update training statistics\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * inputs.size(\u001b[32m0\u001b[39m)\n\u001b[32m     49\u001b[39m _, predicted = torch.max(outputs, \u001b[32m1\u001b[39m)\n\u001b[32m     50\u001b[39m total += labels.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def main() -> None:\n",
    "\t\"\"\"\n",
    "\tMain function to train the model, evaluate it, and display predictions on sample images.\n",
    "\n",
    "\tSteps:\n",
    "\t- Initialize device, data loaders, model, loss function, and optimizer.\n",
    "\t- Train the model.\n",
    "\t- Load the best saved model.\n",
    "\t- Predict emotions for selected sample images and display results.\n",
    "\t\n",
    "\tReturns:\n",
    "\t\tNone\n",
    "\t\"\"\"\n",
    "\tprint(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\t# Create data loaders\n",
    "\ttrain_loader, test_loader = create_datasets()\n",
    "\n",
    "\t# Initialize model\n",
    "\tmodel: EmotionCNN = EmotionCNN().to(DEVICE)\n",
    "\n",
    "\t# Define loss function and optimizer\n",
    "\tcriterion: nn.Module = nn.CrossEntropyLoss()\n",
    "\toptimizer: optim.Optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\t# Train the model\n",
    "\ttrain_model(model, train_loader, test_loader, criterion, optimizer, NUM_EPOCHS)\n",
    "\n",
    "\t# Load best model weights\n",
    "\tmodel.load_state_dict(torch.load('emotion_model.pth', map_location=DEVICE))\n",
    "\n",
    "\t# Test some example images\n",
    "\ttest_images: list = [\n",
    "\t\t'images/train/sad/42.jpg',\n",
    "\t\t'images/train/fear/2.jpg',\n",
    "\t\t'images/train/disgust/299.jpg',\n",
    "\t\t'images/train/happy/7.jpg',\n",
    "\t\t'images/train/surprise/15.jpg'\n",
    "\t]\n",
    "\n",
    "\tfor image_path in test_images:\n",
    "\t\ttrue_label: str = image_path.split('/')[-2]\n",
    "\t\tpredicted_label: str = predict_emotion(model, image_path)\n",
    "\n",
    "\t\t# Load and display the image\n",
    "\t\timg = Image.open(image_path).convert('L')\n",
    "\t\tplt.imshow(img, cmap='gray')\n",
    "\t\tplt.title(f'True: {true_label.capitalize()} | Predicted: {predicted_label.capitalize()}')\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\t# Print the results\n",
    "\t\tprint(f\"Original label : {true_label.capitalize()}\")\n",
    "\t\tprint(f\"Predicted label: {predicted_label.capitalize()}\")\n",
    "\t\tprint('-' * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
